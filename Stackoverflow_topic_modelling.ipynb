{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9Y9Nnr7Svk0",
        "outputId": "ed203a18-f3d2-4ec4-d192-ad88e8057b3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/extended_omw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.corpus import stopwords  #stopwords\n",
        "from nltk.stem import WordNetLemmatizer  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
        "import pandas as pd\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxBbGKj5ShF7",
        "outputId": "6efbd0dc-1d9b-4e7d-fab4-974fa202e4ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "Skipping line 3793: unexpected end of data\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/Questions.csv', encoding='latin-1', engine='python', error_bad_lines=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjFczQnrTIzk"
      },
      "outputs": [],
      "source": [
        "df =df.drop(['OwnerUserId' ,  'CreationDate' , 'ClosedDate' , 'Score'] , axis =1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2Nj1p6eY6tm"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "#nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
        "df['Title']=df['Title'].apply(lambda x : x.lower())\n",
        "df['Title'] = df['Title'].apply(lambda x : ' '.join([w for w in x.split() if not w in stop_words]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYoDSIh8VbVE"
      },
      "outputs": [],
      "source": [
        "doc_clean = [doc.split() for doc in df['Title']]  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(doc_clean, open('dic.pickle', 'wb'))"
      ],
      "metadata": {
        "id": "JKsX8SCXOMQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3vJjYHPVbSO"
      },
      "outputs": [],
      "source": [
        "# Importing Gensim\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
        "dictionary = corpora.Dictionary(doc_clean)\n",
        "#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
        "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHfm1lcHVbPi"
      },
      "outputs": [],
      "source": [
        "# Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "# Running and Trainign LDA model on the document term matrix.\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word = dictionary, passes=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cFm7o3jVbNA",
        "outputId": "6ebf39df-527d-4106-a1e4-68f22a352591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic : 0 \n",
            " Words 0.009*\"variable\" + 0.008*\".net\" + 0.007*\"data\" + 0.007*\"table\" + 0.007*\"-\" + 0.006*\"php\" + 0.006*\"java\" + 0.005*\"menu\" + 0.005*\"last\" + 0.005*\"python\" \n",
            "\n",
            "Topic : 1 \n",
            " Words 0.012*\"way\" + 0.010*\"visual\" + 0.010*\"studio\" + 0.010*\"best\" + 0.008*\"jquery\" + 0.008*\"using\" + 0.007*\"file\" + 0.007*\"change\" + 0.007*\"source\" + 0.007*\"asp.net\" \n",
            "\n",
            "Topic : 2 \n",
            " Words 0.026*\"sql\" + 0.022*\"server\" + 0.011*\"asp.net\" + 0.011*\"get\" + 0.010*\"control\" + 0.008*\"using\" + 0.008*\"c#\" + 0.008*\"service\" + 0.007*\"2005\" + 0.006*\"code\" \n",
            "\n",
            "Topic : 3 \n",
            " Words 0.015*\"using\" + 0.010*\"&\" + 0.009*\"best\" + 0.008*\"user\" + 0.007*\"use\" + 0.007*\"sharepoint\" + 0.005*\"file\" + 0.005*\"php\" + 0.005*\"web\" + 0.005*\"issue\" \n",
            "\n",
            "Topic : 4 \n",
            " Words 0.017*\"using\" + 0.015*\"best\" + 0.010*\"web\" + 0.010*\"java\" + 0.008*\"way\" + 0.007*\"files\" + 0.007*\"database\" + 0.006*\"-\" + 0.006*\"array\" + 0.005*\"application\" \n",
            "\n",
            "Topic : 5 \n",
            " Words 0.020*\"use\" + 0.014*\"way\" + 0.009*\"-\" + 0.009*\"python\" + 0.007*\"best\" + 0.005*\"application\" + 0.005*\"access\" + 0.004*\"code\" + 0.004*\"visual\" + 0.004*\"list\" \n",
            "\n",
            "Topic : 6 \n",
            " Words 0.019*\"c#\" + 0.013*\"way\" + 0.010*\"get\" + 0.009*\"using\" + 0.009*\"string\" + 0.009*\"possible\" + 0.008*\"linq\" + 0.007*\"best\" + 0.006*\"sql\" + 0.005*\"query\" \n",
            "\n",
            "Topic : 7 \n",
            " Words 0.012*\"using\" + 0.009*\"find\" + 0.007*\"server\" + 0.007*\"parent\" + 0.006*\"directory\" + 0.006*\"page\" + 0.006*\"regex\" + 0.005*\"function\" + 0.005*\"creating\" + 0.005*\"way\" \n",
            "\n",
            "Topic : 8 \n",
            " Words 0.015*\".net\" + 0.007*\"one\" + 0.007*\"error\" + 0.007*\"asp.net\" + 0.006*\"way\" + 0.006*\"app\" + 0.006*\"sql\" + 0.006*\"project\" + 0.006*\"file\" + 0.006*\"multiple\" \n",
            "\n",
            "Topic : 9 \n",
            " Words 0.008*\"application\" + 0.008*\"multiple\" + 0.007*\"good\" + 0.006*\"using\" + 0.006*\"vb.net\" + 0.005*\"what's\" + 0.005*\"best\" + 0.005*\"new\" + 0.005*\"javascript\" + 0.005*\"windows\" \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for idx , topic in ldamodel.print_topics(-1):\n",
        "    print(f\"Topic : {idx} \\n Words {topic} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiNNF29KVbJq",
        "outputId": "be2dcab8-19bc-4580-a09a-0a9a45ddbb38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Coherence Score:  0.4880146204343411\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import CoherenceModel\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=ldamodel, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Higher the coherence score , more separable the topics are, better it is . Trying with Multicore Model now -"
      ],
      "metadata": {
        "id": "8ZmI1M4APmAf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LatsDk0VbGX"
      },
      "outputs": [],
      "source": [
        "lda_tfidf = gensim.models.LdaMulticore(doc_term_matrix, num_topics = 10, id2word = dictionary, passes = 15, workers = 4, per_word_topics = True, chunksize = 3000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_tfidf.print_topics(-1):\n",
        "    print(f\"Topic : {idx} \\n Words {topic} \\n\")"
      ],
      "metadata": {
        "id": "PUcRCkKDP9Wp",
        "outputId": "0ac4baa6-3d85-4d77-9eaa-857998070c3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic : 0 \n",
            " Words 0.009*\"multiple\" + 0.009*\".net\" + 0.006*\"class\" + 0.006*\"c#\" + 0.005*\"name\" + 0.005*\"java\" + 0.005*\"using\" + 0.005*\"access\" + 0.004*\"database\" + 0.004*\"find\" \n",
            "\n",
            "Topic : 1 \n",
            " Words 0.018*\"way\" + 0.018*\"best\" + 0.009*\"-\" + 0.008*\"use\" + 0.008*\"what's\" + 0.007*\"java\" + 0.007*\"control\" + 0.006*\"store\" + 0.006*\"asp.net\" + 0.005*\"using\" \n",
            "\n",
            "Topic : 2 \n",
            " Words 0.019*\"server\" + 0.013*\"sql\" + 0.013*\"web\" + 0.013*\"asp.net\" + 0.010*\"file\" + 0.006*\"get\" + 0.005*\"project\" + 0.005*\"good\" + 0.005*\"unit\" + 0.005*\"create\" \n",
            "\n",
            "Topic : 3 \n",
            " Words 0.023*\"using\" + 0.005*\"php\" + 0.005*\"get\" + 0.005*\"java\" + 0.005*\"data\" + 0.004*\".net\" + 0.004*\"page\" + 0.004*\"vs.\" + 0.004*\"vs\" + 0.004*\"call\" \n",
            "\n",
            "Topic : 4 \n",
            " Words 0.008*\"visual\" + 0.007*\"use\" + 0.007*\"get\" + 0.006*\"array\" + 0.006*\"type\" + 0.005*\"file\" + 0.005*\"linq\" + 0.005*\"way\" + 0.005*\"studio\" + 0.005*\"find\" \n",
            "\n",
            "Topic : 5 \n",
            " Words 0.014*\"best\" + 0.011*\"c#\" + 0.010*\"using\" + 0.008*\"web\" + 0.007*\"-\" + 0.007*\"method\" + 0.007*\"application\" + 0.005*\"php\" + 0.005*\"java\" + 0.004*\".net\" \n",
            "\n",
            "Topic : 6 \n",
            " Words 0.013*\"using\" + 0.009*\"way\" + 0.007*\"data\" + 0.007*\"best\" + 0.006*\"visual\" + 0.006*\"use\" + 0.006*\"multiple\" + 0.006*\"windows\" + 0.006*\"source\" + 0.006*\"what's\" \n",
            "\n",
            "Topic : 7 \n",
            " Words 0.010*\"menu\" + 0.007*\"using\" + 0.007*\"application\" + 0.006*\"windows\" + 0.006*\"c++\" + 0.005*\"way\" + 0.005*\"two\" + 0.004*\"without\" + 0.004*\"controls\" + 0.004*\"values\" \n",
            "\n",
            "Topic : 8 \n",
            " Words 0.013*\"get\" + 0.011*\".net\" + 0.011*\"best\" + 0.010*\"use\" + 0.010*\"way\" + 0.008*\"variable\" + 0.008*\"function\" + 0.007*\"sql\" + 0.006*\"-\" + 0.006*\"windows\" \n",
            "\n",
            "Topic : 9 \n",
            " Words 0.013*\"sql\" + 0.013*\"best\" + 0.013*\"server\" + 0.012*\"using\" + 0.010*\"way\" + 0.008*\"file\" + 0.007*\"stored\" + 0.007*\"c#\" + 0.007*\"image\" + 0.006*\"code\" \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_tfidf, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "id": "nGCo-zj_QCnr",
        "outputId": "e43db4ef-afde-4587-b0ed-e6c29bccc656",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Coherence Score:  0.4536393659621747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mrNa9WnoQKc1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Stackoverflow topic modelling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}